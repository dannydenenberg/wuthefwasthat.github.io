---
layout: post
title:  "Security in a world of AI"
date:   2016-06-29 10:18:32 -0700
categories: AI security
comments: true
---

As time goes on, it seems we will put increasingly more power in the hands of intelligent machine systems.
Many people, including myself, have an intuition that this will be dangerous, and pose a risk to humanity.
Much of the focus in this area is on a time in which we've acheived artificial general intelligence, and have machines capable of exceeding human capability on nearly all tasks (e.g. engineering, science research, art, showing empathy, etc).

Since a post-AGI world is difficult to reason about, I'd like to instead think about a world not too far from the current one.
In the near future, I think companies will increasingly run intelligent AIs that effectively make decisions for them or their customers.
For example, perhaps:

- Cars mostly drive themselves, with only slight supervision from humans.
- Google search rankings are personalized and powered by neural networks.
- SAT and college essays are read by a neural network, and given a grade.
  Humans then take a final cursory pass before accepting, although the AI does well enough
  that this is mainly for safeguarding/defensibility.
- News articles are synthesized by AI systems that process rough notes that journalists jot down.
  The journalist then edits the resulting synthesized article.
- 

If these AI systems work well, we will trust them more and more with important decisions.
One of the main ways I foresee we could lose this trust is if the systems aren't secure to hackers (motivated by normal human motivations).

Here are some ways I imagine this could go wrong:

I'd like to spend some time imagining what an adversary would do, in a world not too far from the current one, 
- Denial of service
- Hacking databases/obtaining info
- Request forgery
- Poisoning data
- Adversarial examples
- Undetectable backdoors
