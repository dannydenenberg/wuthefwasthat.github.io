## Free will

### Free will is not about technically possible outcomes

How should we find meaning in talking about free will in a deterministic universe?
Say you're presented with choices A (to eat) and B (not to eat) in some particular situation.
If according to the universe's laws, you always choose A (as I only do), what would it mean to say you could have chosen either?
Even if there are, say, splitting multiverses, if you don’t get too choose the splitting behavior and distribution, you still didn't really have a choice - you just followed the instruction of God's dice.
Any counterargument should be able to also explain why a photon passing through a polarising filter doesn't have free will.

### Free will is a property of the decision-making algorithm

Imagine a robot which behaved in the same way as you, in every situation, but it did so by having a giant lookup table from situations to actions.
I'm going to assume you wouldn't think of such a robot as having free will.
This shows free will is not purely a property of input-output behavior.

### Hypothesis

Given these two statements to work with, there is a simple hypothesis which jumps out to me:  Free will is experienced by an algorithm when it can consider making different choices.

Alright, so does this mean that a tree search algorithm has free will?

## Consciousness

I think many people reject this obvious line of thought, because of the feeling that they have free will, which lets them make decisions, or consciousness, which gives them experiences.

But is it so strange for a computer-simulated human to truly feel like a human? But more generally, can any algorithm feel conscious, and like it has will to make decisions?

Well, a multiplication algorithm probably doesn’t “feel” like it is doing multiplication.
And a shortest path algorithm probably doesn’t “feel” like it is performing shortest path.
When a doctor uses that reflex hammer on your knee to make you kick our, you're reacting, not deciding.
The same applies if a bag of chips is lying on the table, and you keep reflexively eating them without thinking about it.
A calculator probably experiences a bunch of knee-jerk reactions that amount to things like multiplication of ten digit numbers.
If your whole life was also lived as a bunch of reactions, would you really feel conscious?

So we should wonder – for humans, what is the difference between feeling and not feeling like you had a decision?
I suspect the answer has to do with self-awareness.
The decisions that don’t feel like part of our free will are precisely the ones we aren’t actively deciding about.
And intuitively, if I wasn’t aware of myself as a whole, I wouldn’t feel conscious at all.
In fact, I think humans are perfectly capable of becoming less “conscious”.
For example, I suspect most people feel a lack of free will and consciousness while dreaming.
An exception is lucid dreamers, who are aware that they are dreaming.
Furthermore, when we are babies, we seem to be significantly less conscious and self-aware.

So the issue seems to be that the algorithms mentioned were unaware of what they were doing.
Let’s put ourselves in an algorithm’s shoes.
If you spent a year doing nothing but multiplying 10 digit numbers, and had no self-reflection, you indeed might feel exactly like a calculator, with no sense of free will or consciousness.
Now we get to try to fit an algorithm in our shoes.
Recall the program which was weighing options A and B.
Suppose it is also aware of its own interaction with the environment through its input/output behavior, and considers heuristics or even simulation to predict happen in the environment if it were to output A (it gets fat), and if it were to output B (it gets hungry).
As it thinks about the counterfactuals about its own decisions, perhaps it feels somewhat like it has a choice!
More generally, it seems quite plausible that any program which is able to reason about its own existence might feel conscious.
